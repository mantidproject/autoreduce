#!/usr/bin/env python
VERSION = "1.4.2"

from suds.client import Client

import nxs, os, numpy, sys, posixpath
import xml.utils.iso8601, ConfigParser
from datetime import datetime
from nxs import NeXusError

def usage():
    print 'Usage: ingestNexus <infilename> <hostAndPort> <password>'
    print 'Example: ingestNexus PG3_10678_event.nxs orion.sns.gov:8181 password'
    sys.exit(-1)

def ingestNexus(infilename, sessionId, service, factory):
  
    #find facility, investigation_type 
    config = ConfigParser.RawConfigParser()
    config.read('/etc/autoreduce/icat4.cfg')

    #open nexus file
    file = nxs.open(infilename, 'r')
    try:
        file.opengroup('entry')
    except NeXusError:
        print "Couldn't open entry"
        try:
            file.opengroup('entry-Off_Off')
            print "Opened entry-Off_Off"
        except:
            print "except"
    except:
        print('except')

    investigation = factory.create("investigation")

    #find facility, investigation_type 
    facility = factory.create("facility")
    facility.id = config.get('Facility', 'sns')
    investigation.facility = facility
        
    invType = factory.create("investigationType")
    invType.id = config.get('InvestigationType', 'experiment')
    investigation.type = invType 
  
    #investigation name 
    file.opendata('experiment_identifier')
    investigation.name = file.getdata()
    file.closedata()

    #investigation run number 
    file.opendata('collection_identifier')
    investigation.visitId = file.getdata()
    file.closedata()

    #investigation title
    file.opendata('title')
    investigation.title = file.getdata()
    file.closedata()
    
    #set dataset name 
    dataset = factory.create("dataset")

    #dataset run number
    file.opendata('run_number')
    dataset.name = file.getdata() 
    file.closedata()

    #dataset notes 
    file.opendata('title')
    dataset.description = file.getdata()
    file.closedata()
    
    #dataset notes 
    file.opendata('notes')
    notes = file.getdata()
    file.closedata()
    
    dsType = factory.create("datasetType")
    dsType.id = config.get('DatasetType', 'experiment_raw')
    dataset.type = dsType
    
    #set dataset start time 
    file.opendata('start_time')
    dataset.startDate = file.getdata()
    file.closedata()

    #set dataset end time 
    file.opendata('end_time')
    dataset.endDate = file.getdata()
    file.closedata()

    #dataset proton_charge 
    file.opendata('proton_charge')
    protonCharge = file.getdata()
    file.closedata()

    #dataset total_counts 
    file.opendata('total_counts')
    totalCounts = file.getdata()
    file.closedata()

    #dataset duration 
    file.opendata('duration')
    duration = file.getdata()
    file.closedata()

    #investigation instrument 
    file.opengroup('instrument')
    file.opendata('name')
    for attr,value in file.attrs():
        if attr == 'short_name':
            instrument = factory.create("instrument")
            instrument.name = value 
            instrument.id = config.get('Instrument', value.lower())
            investigation.instrument = instrument 
    file.closedata()
    file.closegroup()

    run_path = posixpath.abspath(posixpath.join(infilename, '../..'))
    dataset.location = run_path
    datafiles = []

    #set dataset parameters
    parameters = []

    #1) parameter proton_charge 
    if protonCharge:
        parameterType = factory.create("parameterType")
        parameterType.id = config.get('ParameterType', 'proton_charge')
        parameterType.applicableToDataset = config.getboolean('ParameterType', 'proton_charge_applicable_to_dataset')
        datasetParameter = factory.create("datasetParameter")
        datasetParameter.type = parameterType
        datasetParameter.numericValue = protonCharge 
        parameters.append(datasetParameter)

    #2) parameter total_counts 
    if totalCounts:
        parameterType = factory.create("parameterType")
        parameterType.id = config.get('ParameterType', 'total_counts')
        parameterType.applicableToDataset = config.getboolean('ParameterType', 'total_counts_applicable_to_dataset')
        datasetParameter = factory.create("datasetParameter")
        datasetParameter.type = parameterType 
        datasetParameter.numericValue = totalCounts
        parameters.append(datasetParameter)

    #3) parameter duration 
    if duration:
        parameterType = factory.create("parameterType")
        parameterType.id = config.get('ParameterType', 'duration')
        parameterType.applicableToDataset = config.getboolean('ParameterType', 'duration_applicable_to_dataset')
        datasetParameter = factory.create("datasetParameter")
        datasetParameter.type = parameterType 
        datasetParameter.numericValue = duration 
        parameters.append(datasetParameter)

    #4) parameter notes
    if notes:
        parameterType = factory.create("parameterType")
        parameterType.id = config.get('ParameterType', 'notes')
        parameterType.applicableToDataset = config.getboolean('ParameterType', 'notes_applicable_to_dataset')
        datasetParameter = factory.create("datasetParameter")
        datasetParameter.type = parameterType 
        datasetParameter.stringValue = notes 
        parameters.append(datasetParameter)

    dataset.parameters = parameters

    for dirpath, dirnames, filenames in os.walk(run_path):
        for filename in [f for f in filenames]:
            if filename.startswith('.') != True:
                datafile = factory.create("datafile")
                filepath = os.path.join(dirpath,filename)
                extension = os.path.splitext(filename)[1][1:]
                datafile.name = filename
                datafile.location = filepath
                dfFormat = factory.create("datafileFormat")
                dfFormat.id = config.get('DatafileFormat', extension)
                datafile.datafileFormat = dfFormat 
                modTime = os.path.getmtime(filepath)
                datafile.datafileCreateTime = xml.utils.iso8601.tostring(modTime)
                datafile.fileSize = os.path.getsize(filepath)

                datafiles.append(datafile)

    dataset.datafiles = datafiles

    samples = []

    for a ,nxclass in file.entries():

        if nxclass == "NXsample":
            sample = factory.create("sample")
            file.opendata('name')
            sample.name = file.getdata()
            file.closedata()

            sampleParameters = []

            #set sample nature
            file.opendata('nature')
            nature = file.getdata()
            file.closedata()
            if nature:       
                parameterType = factory.create("parameterType")
                parameterType.id = config.get('ParameterType', 'nature')
                parameterType.applicableToSample = config.getboolean('ParameterType', 'nature_applicable_to_sample')
                sampleParameter = factory.create("sampleParameter")
                sampleParameter.type = parameterType
                sampleParameter.stringValue = nature 
                sampleParameters.append(sampleParameter)
        
            file.opendata('identifier')
            identifier = file.getdata()
            file.closedata()
  
            if identifier:
                parameterType = factory.create("parameterType")
                parameterType.id = config.get('ParameterType', 'identifier')
                parameterType.applicableToSample = config.getboolean('ParameterType', 'identifier_applicable_to_sample')
                sampleParameter = factory.create("sampleParameter")
                sampleParameter.type = parameterType
                sampleParameter.stringValue = identifier
                sampleParameters.append(sampleParameter)
        
            sample.parameters = sampleParameters
            samples.append(sample)

        if nxclass == "NXuser":
            a = "a"
            #investigationUser = setInvestigationUser(file, sessionId, factory, service)
            #investigationUsers.append(investigationUser)


    file.closegroup()
    file.close()
    
    dbDatasets = service.search(sessionId, "Dataset INCLUDE Datafile [name = '" + str(dataset.name) + "'] <-> Investigation <-> Instrument [name = '" + str(instrument.name) + "'] <-> DatasetType [name = 'experiment_raw']")

    if len(dbDatasets) == 0:
    
        dbInvestigations = service.search(sessionId, "Investigation INCLUDE Sample [name = '" + str(investigation.name) + "'] <-> Instrument [name = '" + instrument.name + "']")
        
        if len(dbInvestigations) == 0: 
            print "New IPTS: creating investigation, sample, run..."
            # create new investigation
            invId = service.create(sessionId, investigation)
            investigation.id = invId
            print "  invId: %s"%(str(invId))
            
            # create new sample
            sample.investigation = investigation
            sampleId = service.create(sessionId, sample)
            sample.id = sampleId
            print "  sampleId: %s"%(str(sampleId))
        
        elif len(dbInvestigations) == 1:
            investigation = dbInvestigations[0]
            dbSamples = investigation.samples
            
            newSample = True
            for dbSample in dbSamples:
                if dbSample.name == sample.name:
                    sample.id = dbSample.id
                    newSample = False
            
            if newSample == True:
                print "New run: existing investigation, creating sample and run..."
                sample.investigation = investigation
                sampleId = service.create(sessionId, sample)
                sample.id = sampleId
            else:
                print "New run: existing investigation and sample, creating run..."
            
        else:
            print "ERROR, there should be only one investigation per instrument per investigation name"  

        # create new dataset
        dataset.sample = sample
        dataset.investigation = investigation
        datasetId = service.create(sessionId, dataset)
        print "  datasetId: %s"%(str(datasetId))
            
    elif len(dbDatasets) == 1:
    
        print "Run %s is already cataloged, updating catalog..."%(dataset.name)
        
        dbDataset = dbDatasets[0]
        print "  datasetId: %s"%(str(dbDataset.id))
        
        # update "one to many" relationships
        
        if hasattr(dbDataset, "datafiles"):
            dfs = getattr(dbDataset, "datafiles")
            service.deleteMany(sessionId, dfs)
            
        for df in datafiles:
 		    df.dataset = dbDataset
        service.createMany(sessionId, datafiles)
        
        # update "many to one" relationships
        
        ds = service.get(sessionId, "Dataset INCLUDE 1", dbDataset.id)
        print "  ds: %s"%(str(ds))
        
        investigation.id = ds.investigation.id
        
        dbSamples = service.search(sessionId, "Sample <-> Investigation [id = '" + str(ds.investigation.id) + "']")
        updateSample = True
        for sa in dbSamples:
            if sa.name == sample.name:
                sample = sa
                updateSample = False
                print "  sample: %s"%(str(sample))
             
        if updateSample == True:
            sample.id = ds.sample.id
            sample.investigation = investigation
            service.update(sessionId, sample)
        
        dataset.id = ds.id
        dataset.sample = sample
        dataset.investigation = investigation   
        
        service.update(sessionId, dataset)
        service.update(sessionId, investigation)

        
    else:
        print "ERROR, there should be only one dataset per run number per type experiment_raw" 

    print "INVESTIGATION:"
    print "  NAME: %s"%(str(investigation.name))

    print "DATASET:"
    print "  RUN NUMBER: %s"%(str(dataset.name))
    print "  TITLE: %s"%(str(dataset.description))
    print "  START TIME: %s"%(str(dataset.startDate))
    print "  END TIME: %s"%(str(dataset.endDate))

    '''for datafile in dataset.datafiles:
        print "DATAFILE:"
        print "  NAME: %s"%(str(datafile.name))
        print "  LOCATION: %s"%(str(datafile.location))'''

    print "SAMPLE: "
    print "  NAME: %s"%(str(sample.name))

    return investigation

def main(argv):
    args = sys.argv[1:]
    if len(args) != 4:
        usage()

    infilename = args[0]
    plugin = args[1]
    hostAndPort = args[2]
    password = args[3]

    client = Client("https://" + hostAndPort + "/ICATService/ICAT?wsdl")
    service = client.service
    factory = client.factory

    credentials = factory.create("credentials")
    entry = factory.create("credentials.entry")
    entry.key = "username"
    entry.value = "root" 
    credentials.entry.append(entry)
    entry = factory.create("credentials.entry")
    entry.key = "password"
    entry.value = password
    credentials.entry.append(entry)
    
    print "Begin login at: ", str(datetime.now()) 
    sessionId = service.login(plugin, credentials)
    print "End login at: ", str(datetime.now()) 

    print "Begin ingestNexus at: ", str(datetime.now()) 
    investigation = ingestNexus(infilename, sessionId, service, factory)
    print "End ingestNexus at: ", str(datetime.now()) 

    print "Begin logout at: ", str(datetime.now()) 
    service.logout(sessionId)
    print "End logout at: ", str(datetime.now()) 

if __name__ == "__main__":
   main(sys.argv[1:])
